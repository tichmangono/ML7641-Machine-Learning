{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE - 7641 Machine Learning: Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should implement five learning algorithms. They are for:\n",
    "\n",
    "* Decision trees with some form of pruning*\n",
    "* Neural networks*\n",
    "* Boosting*\n",
    "* Support Vector Machines*\n",
    "* k-nearest neighbors*\n",
    "\n",
    "Each algorithm is described in detail in your textbook, the handouts, and all over the web. In fact, instead of implementing the algorithms yourself, you may (and by may I mean should) use software packages that you find elsewhere; however, if you do so you should provide proper attribution. Also, you will note that you have to do some fiddling to get good results, graphs and such, so even if you use another's package, you may need to be able to modify it in various ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, ExtraTreeClassifier\n",
    "from sklearn.ensemble import  RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score, GridSearchCV, KFold, GroupKFold, GroupShuffleSplit, learning_curve\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import explained_variance_score, make_scorer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(121)\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Get the data sets\n",
    "from sklearn.datasets import load_digits, load_breast_cancer\n",
    "data_img, data_cancer = load_digits(), load_breast_cancer()\n",
    "print(data_img.keys(), \"\\n\",data_cancer.keys())\n",
    "\n",
    "x1, y1 = pd.DataFrame(data_img[\"data\"]), pd.Series(data_img[\"target\"])\n",
    "x2, y2 = pd.DataFrame(data_cancer[\"data\"],columns=data_cancer.feature_names), pd.Series(data_cancer[\"target\"])\n",
    "\n",
    "# shuffle the indices first\n",
    "n1 = np.random.RandomState(seed=121).permutation(range(len(x1)))\n",
    "n2 =np.random.RandomState(seed=121).permutation(range(len(x2)))\n",
    "\n",
    "# Split indices\n",
    "t1 ,v1 , r1 = x1.iloc[-400:,:].index, x1.iloc[-800:-400,:].index, x1.iloc[:-800,:].index\n",
    "t2 ,v2 , r2 = x2.iloc[-114:,:].index, x2.iloc[-228:-114,:].index, x2.iloc[:-228,:].index\n",
    "print(t1, v1, r1)\n",
    "print(t2, v2, r2)\n",
    "\n",
    "y1.hist(bins=10)\n",
    "\n",
    "y2.hist(bins=10)\n",
    "\n",
    "# dataset splits\n",
    "x1_ts, x1_val, x1_tr = x1.iloc[t1,:], x1.iloc[v1,:], x1.iloc[r1,:]\n",
    "y1_ts, y1_val, y1_tr = y1[t1], y1[v1], y1[r1]\n",
    "x2_ts, x2_val, x2_tr = x2.iloc[t2,:], x2.iloc[v2,:], x2.iloc[r2,:]\n",
    "y2_ts, y2_val, y2_tr = y2[t2], y2[v2], y2[r2]\n",
    "\n",
    "x1_ = x1.iloc[list(r1)+list(v1),:]\n",
    "y1_ = y1[list(r1)+list(v1)]\n",
    "x2_ = x2.iloc[list(r2)+list(v2),:]\n",
    "y2_ = y2[list(r2)+ list(v2)]\n",
    "print(len(x1_), len(y1_), len(x2_), len(y2_))\n",
    "\n",
    "def plot_curve(c, dataset):\n",
    "    # instantiate\n",
    "    clf = c[1]\n",
    "    # fit\n",
    "    clf.fit(X, y)\n",
    "      \n",
    "    train_sizes, train_scores, test_scores = learning_curve(clf, X, y, n_jobs=-1, cv=cv, scoring=\"accuracy\",\n",
    "                                                            train_sizes=np.linspace(.1, 1.0, 5), verbose=0)\n",
    "\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(c[0]+ \" on Dataset: \"+ dataset)\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    # box-like grid\n",
    "    plt.grid()\n",
    "    \n",
    "    # plot the std deviation as a transparent range at each training set size\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    \n",
    "    # plot the average training and test score lines at each training set size\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    \n",
    "    # sizes the window for readability and displays the plot\n",
    "    # shows error from 0 to 1.1\n",
    "    plt.ylim(-.1,1.1)\n",
    "    plt.show()\n",
    "\n",
    "def set_data(dataset):\n",
    "    if dataset==\"digits\":\n",
    "        dataset=\"digits\"\n",
    "        X, y =x1_, y1_\n",
    "    else:\n",
    "        dataset=\"diagnosis\"\n",
    "        X, y =x2_, y2_\n",
    "    return X, y, dataset\n",
    "\n",
    "#size = len(r1)\n",
    "cv = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "c1 = [\"DecisionTreeClassifier\", DecisionTreeClassifier(random_state=121)]\n",
    "c2 = [\"MLPClassifier\", MLPClassifier(random_state=121)]\n",
    "c3 = [\"AdaBoostClassifier\", AdaBoostClassifier(random_state=121)]\n",
    "c4 = [\"Suppor Vector Machine - Classifier\", SVC(random_state=121)]\n",
    "c5 = [\"KNeighborsClassifier\", KNeighborsClassifier()]\n",
    "\n",
    "par = [1, 3, 5, 7, 15, 21, 35, 50]\n",
    "c1b = [\"DecisionTreeClassifier\", DecisionTreeClassifier(random_state=121, min_samples_leaf=k)]\n",
    "\n",
    "X, y, dataset = set_data(\"\")\n",
    "results=[]\n",
    "for i in range(1,21,2):\n",
    "    clf = DecisionTreeClassifier(max_depth=i, random_state=121)\n",
    "    # Perform 3-fold cross validation \n",
    "    scores = cross_val_score(estimator=clf, X=X, y=y, cv=3, n_jobs=-1)\n",
    "    results.append(scores.mean())\n",
    "#print(results)\n",
    "\n",
    "plt.plot(range(1,21,2), results)\n",
    "plt.title(\"Pruned Decision Tree on \"+ dataset)\n",
    "plt.ylim(0.8,1)\n",
    "plt.xlabel(\"Max Tree Depth\")\n",
    "plt.ylabel(\"Accuracy Score\")\n",
    ";\n",
    "\n",
    "X, y, dataset = set_data(\"digits\")\n",
    "results=[]\n",
    "for i in range(1,21,2):\n",
    "    clf = DecisionTreeClassifier(min_samples_leaf=i, random_state=121)\n",
    "    # Perform 3-fold cross validation \n",
    "\n",
    "\n",
    "c2b = [\"MLPClassifier\", MLPClassifier(random_state=121)]\n",
    "\n",
    "X, y, dataset = set_data(\"\")\n",
    "results=[]\n",
    "for i in range(1,200,20):\n",
    "    clf = MLPClassifier(random_state=121, hidden_layer_sizes=(i,))\n",
    "    # Perform 3-fold cross validation \n",
    "    scores = cross_val_score(estimator=clf, X=X, y=y, cv=3, n_jobs=-1)\n",
    "    results.append(scores.mean())\n",
    "#print(results)\n",
    "\n",
    "plt.plot(range(1,200,20), results)\n",
    "plt.title(\"MLPClassifier on \"+ dataset)\n",
    "plt.ylim(0,1)\n",
    "plt.xlabel(\"Number of Neurons in Hidden Layer\")\n",
    "plt.ylabel(\"Accuracy Score\")\n",
    ";\n",
    "\n",
    "X, y, dataset = set_data(\"digits\")\n",
    "results=[]\n",
    "for i in range(1,200,20):\n",
    "    clf = MLPClassifier(random_state=121, hidden_layer_sizes=(i,))\n",
    "    # Perform 3-fold cross validation \n",
    "    scores = cross_val_score(estimator=clf, X=X, y=y, cv=3, n_jobs=-1)\n",
    "    results.append(scores.mean())\n",
    "#print(results)\n",
    "\n",
    "plt.plot(range(1,200,20), results)\n",
    "plt.title(\"MLPClassifier on \"+ dataset)\n",
    "plt.ylim(0,1)\n",
    "plt.xlabel(\"Number of Neurons in Hidden Layer\")\n",
    "plt.ylabel(\"Accuracy Score\")\n",
    ";\n",
    "\n",
    "c3b = [\"AdaBoostClassifier\", AdaBoostClassifier(random_state=121)]\n",
    "c4b = [\"Suppor Vector Machine - Classifier\", SVC(random_state=121)]\n",
    "c5b = [\"KNeighborsClassifier\", KNeighborsClassifier()]\n",
    "\n",
    "#**Decision Trees.** For the decision tree, you should implement or steal a decision tree algorithm (and by \"implement or steal\" I mean \"steal\"). Be sure to use some form of pruning. You are not required to use information gain (for example, there is something called the GINI index that is sometimes used) to split attributes, but you should describe whatever it is that you do use.\n",
    "\n",
    "X, y, dataset = set_data(\"digits\")\n",
    "a=plot_curve(c1, dataset)\n",
    "X, y, dataset = set_data(\"\")\n",
    "b=plot_curve(c1, dataset)\n",
    "\n",
    "#**Neural Networks.** For the neural network you should implement or steal your favorite kind of network and training algorithm. You may use networks of nodes with as many layers as you like and any activation function you see fit.\n",
    "\n",
    "X, y, dataset = set_data(\"digits\")\n",
    "plot_curve(c2, dataset)\n",
    "X, y, dataset = set_data(\"\")\n",
    "plot_curve(c2, dataset)\n",
    "\n",
    "#**Boosting.** Implement or steal a boosted version of your decision trees. As before, you will want to use some form of pruning, but presumably because you're using boosting you can afford to be much more aggressive about your pruning.\n",
    "\n",
    "X, y, dataset = set_data(\"digits\")\n",
    "plot_curve(c3, dataset)\n",
    "X, y, dataset = set_data(\"\")\n",
    "plot_curve(c3, dataset)\n",
    "\n",
    "#**Support Vector Machines.** You should implement (for sufficently loose definitions of implement including \"download\") SVMs. This should be done in such a way that you can swap out kernel functions. I'd like to see at least two.\n",
    "\n",
    "#clf = SVC(gamma='auto')\n",
    "#clf.fit(X, y) \n",
    "\n",
    "#print(clf.predict([[-0.8, -1]]))\n",
    "X, y, dataset = set_data(\"digits\")\n",
    "plot_curve(c4, dataset)\n",
    "X, y, dataset = set_data(\"\")\n",
    "plot_curve(c4, dataset)\n",
    "\n",
    "cc1=[\"Suppor Vector Machine - Classifier Linear\", SVC(random_state=121, kernel=\"linear\")]\n",
    "X, y, dataset = set_data(\"digits\")\n",
    "plot_curve(cc1, dataset)\n",
    "X, y, dataset = set_data(\"\")\n",
    "plot_curve(cc1, dataset)\n",
    "\n",
    "cc1=[\"Suppor Vector Machine - Classifier Linear\", SVC(random_state=121, kernel=\"linear\", degree=1)]\n",
    "X, y, dataset = set_data(\"digits\")\n",
    "plot_curve(cc1, dataset)\n",
    "X, y, dataset = set_data(\"\")\n",
    "plot_curve(cc1, dataset)\n",
    "\n",
    "cc1=[\"Suppor Vector Machine - Classifier Linear\", SVC(random_state=121, deg)]\n",
    "X, y, dataset = set_data(\"digits\")\n",
    "plot_curve(cc1, dataset)\n",
    "X, y, dataset = set_data(\"\")\n",
    "plot_curve(cc1, dataset)\n",
    "\n",
    "#**k-Nearest Neighbors.** You should \"implement\" (the quotes mean I don't mean it: steal the code) kNN. Use different values of k.\n",
    "\n",
    "#neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "#neigh.fit(X, y) \n",
    "\n",
    "X, y, dataset = set_data(\"digits\")\n",
    "plot_curve(c5, dataset)\n",
    "X, y, dataset = set_data(\"\")\n",
    "plot_curve(c5, dataset)\n",
    "\n",
    "\n",
    "# Indian Liver Data\n",
    "# Load_digits (MNIST dataset)\n",
    "\n",
    "# How do you learn the model, Steps for each model you are learning\n",
    "# Learning Curve Analysis\n",
    "# Model Complexity Analysis\n",
    "# Performance you see for each method/model and why? -  Metrics Accuracy, AUC, Confusion Matrix, Precision and Recall \n",
    "# Finally, do a comparison across everything and then summary on what you saw\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
