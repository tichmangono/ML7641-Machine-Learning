{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning & Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import KMeans\n",
    "from sklearn.cluster import KMeans\n",
    "# Create a KMeans instance with 3 clusters: model\n",
    "model = KMeans(n_clusters=3)\n",
    "# Fit model to points\n",
    "model.fit(X_norm)\n",
    "# Determine the cluster labels of new_points: labels\n",
    "labels = model.labels_\n",
    "# Print cluster labels of new_points\n",
    "l b l\n",
    "Version 3\n",
    " 3 commits\n",
    "Data\n",
    "Log\n",
    "Comments\n",
    "Notebook\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectation Maximization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "gmm = GaussianMixture(n_components = 2, tol=0.000001)\n",
    "gmm.fit(np.expand_dims(data, 1)) # Parameters: array-like, shape (n_samples, n_features), 1 dimension dataset so 1 feature\n",
    "Gaussian_nr = 1\n",
    "for mu, sd, p in zip(gmm.means_.flatten(), np.sqrt(gmm.covariances_.flatten()), gmm.weights_):\n",
    "    print('Gaussian {:}: μ = {:.2}, σ = {:.2}, weight = {:.2}'.format(Gaussian_nr, mu, sd, p))\n",
    "    g_s = stats.norm(mu, sd).pdf(x) * p\n",
    "    plt.plot(x, g_s, label='gaussian sklearn');\n",
    "    Gaussian_nr += 1\n",
    "sns.distplot(data, bins=20, kde=False, norm_hist=True)\n",
    "gmm_sum = np.exp([gmm.score_samples(e.reshape(-1, 1)) for e in x]) #gmm gives log probability, hence the exp() function\n",
    "plt.plot(x, gmm_sum, label='gaussian mixture');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dim reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 2 first PCs\n",
    "X_centered = X - X.mean(axis=0) # PCA assumes data is centered\n",
    "U, s, V = np.linalg.svd(X_centered)\n",
    "c1 = V.T[:, 0]\n",
    "c2 = V.T[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project to d dims , 2\n",
    "W2 = V.T[:, :2]\n",
    "X2D = X_centered.dot(W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using sklearn - its does the data centering for you\n",
    "pca = PCA(n_components = 2)\n",
    "X2D = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get first PC\n",
    "pca.components_.T[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explained variance\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing right number of dims\n",
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "d = np.argmax(cumsum >= 0.95) + 1\n",
    "\n",
    "# OR\n",
    "pca = PCA(n_components=0.95)\n",
    "X_reduced = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA for compression\n",
    "pca = PCA(n_components = 154)\n",
    "X_mnist_reduced = pca.fit_transform(X_mnist)\n",
    "X_mnist_recovered = pca.inverse_transform(X_mnist_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomized PCA\n",
    "rnd_pca = PCA(n_components=154, svd_solver=\"randomized\")\n",
    "X_reduced = rnd_pca.fit_transform(X_mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Generate sample data\n",
    ">>> import numpy as np\n",
    ">>> from scipy import signal\n",
    ">>> time = np.linspace(0, 10, 2000)\n",
    ">>> s1 = np.sin(2 * time) # Signal 1 : sinusoidal signal\n",
    ">>> s2 = np.sign(np.sin(3 * time)) # Signal 2 : square signal\n",
    ">>> s3 = signal.sawtooth(2 * np.pi * time) # Signal 3: saw tooth signal\n",
    ">>> S = np.c_[s1, s2, s3]\n",
    ">>> S += 0.2 * np.random.normal(size=S.shape) # Add noise\n",
    ">>> S /= S.std(axis=0) # Standardize data\n",
    ">>> # Mix data\n",
    ">>> A = np.array([[1, 1, 1], [0.5, 2, 1], [1.5, 1, 2]]) # Mixing matrix\n",
    ">>> X = np.dot(S, A.T) # Generate observations\n",
    ">>> # Compute ICA\n",
    ">>> ica = decomposition.FastICA()\n",
    ">>> S_ = ica.fit_transform(X) # Get the estimated sources\n",
    ">>> A_ = ica.mixing_.T\n",
    ">>> np.allclose(X, np.dot(S_, A_) + ica.mean_)\n",
    "True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Any other feature selection algorithm you desire "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
